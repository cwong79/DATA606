---
title: "DATA 606 Fall 2018 - Final Exam"
author: 'Calvin Wong'
output:
  html_document:
    df_print: paged
---

```{r, echo=FALSE}
options(digits = 2)
```

# Part I

Please put the answers for Part I next to the question number (2pts each):

1.  B
2.  D
3.  D
4.  D
5.  B
6.  D

7a. Describe the two distributions (2pts).

A - Right-skewed, unimodal, small spread
B - Symmetrical, unimodal, wide spread

7b. Explain why the means of these two distributions are similar but the standard deviations are not (2 pts).

These two distributions are similar because B represents the distribution of the mean from A. Random sampling has elements of sampling error, however, natural suggestion dictates there is equality among any single selection. Therefore, the means of A and B should be similar. The standard deviations are not similar because the figures represents two different ideas. Figure A represents the distribution of an observed variable, wherelse, Figure B below represents the distribution of the means. Therefore, the standard deviation of A is the variation of that observed variable but the standard deviation of B is the variation of means of a sample size. 

7c. What is the statistical principal that describes this phenomenon (2 pts)?

The Central Limit Theorem is the principal that describes this. It states that properly normalized sums tends to follow a normal distribution.

# Part II

Consider the four datasets, each with two columns (x and y), provided below. Be sure to replace the `NA` with your answer for each part (e.g. assign the mean of `x` for `data1` to the `data1.x.mean` variable). When you Knit your answer document, a table will be generated with all the answers.

```{r}
options(digits=2)
data1 <- data.frame(x=c(10,8,13,9,11,14,6,4,12,7,5),
					y=c(8.04,6.95,7.58,8.81,8.33,9.96,7.24,4.26,10.84,4.82,5.68))
data2 <- data.frame(x=c(10,8,13,9,11,14,6,4,12,7,5),
					y=c(9.14,8.14,8.74,8.77,9.26,8.1,6.13,3.1,9.13,7.26,4.74))
data3 <- data.frame(x=c(10,8,13,9,11,14,6,4,12,7,5),
					y=c(7.46,6.77,12.74,7.11,7.81,8.84,6.08,5.39,8.15,6.42,5.73))
data4 <- data.frame(x=c(8,8,8,8,8,8,8,19,8,8,8),
					y=c(6.58,5.76,7.71,8.84,8.47,7.04,5.25,12.5,5.56,7.91,6.89))
```

For each column, calculate (to two decimal places):

#### a. The mean (for x and y separately; 1 pt).

```{r include=TRUE}
data1.x.mean <- mean(data1$x)
data1.y.mean <- mean(data1$y)
data2.x.mean <- mean(data2$x)
data2.y.mean <- mean(data2$y)
data3.x.mean <- mean(data3$x)
data3.y.mean <- mean(data3$y)
data4.x.mean <- mean(data4$x)
data4.y.mean <- mean(data4$y)
```

#### b. The median (for x and y separately; 1 pt).

```{r include=TRUE}
data1.x.median <- median(data1$x)
data1.y.median <- median(data1$y)
data2.x.median <- median(data2$x)
data2.y.median <- median(data2$y)
data3.x.median <- median(data3$x)
data3.y.median <- median(data3$y)
data4.x.median <- median(data4$x)
data4.y.median <- median(data4$y)
```

#### c. The standard deviation (for x and y separately; 1 pt).

```{r include=TRUE}
data1.x.sd <- sd(data1$x)
data1.y.sd <- sd(data1$y)
data2.x.sd <- sd(data2$x)
data2.y.sd <- sd(data2$y)
data3.x.sd <- sd(data3$x)
data3.y.sd <- sd(data3$y)
data4.x.sd <- sd(data4$x)
data4.y.sd <- sd(data4$y)
```

#### For each x and y pair, calculate (also to two decimal places; 1 pt):

#### d. The correlation (1 pt).

```{r include=TRUE}
data1.correlation <- cor(data1$x, data1$y)
data2.correlation <- cor(data2$x, data2$y)
data3.correlation <- cor(data3$x, data3$y)
data4.correlation <- cor(data4$x, data4$y)
```

#### e. Linear regression equation (2 pts).

```{r include=TRUE}
data1_lm <- lm(y ~ x, data1)
data2_lm <- lm(y ~ x, data2)
data3_lm <- lm(y ~ x, data3)
data4_lm <- lm(y ~ x, data4)

data1.slope <- data1_lm$coefficients[2]
data2.slope <- data2_lm$coefficients[2]
data3.slope <- data3_lm$coefficients[2]
data4.slope <- data4_lm$coefficients[2]

data1.intercept <- data1_lm$coefficients[1]
data2.intercept <- data2_lm$coefficients[1]
data3.intercept <- data3_lm$coefficients[1]
data4.intercept <- data4_lm$coefficients[1]
```

#### f. R-Squared (2 pts).

```{r include=TRUE}
data1.rsquared <- data1.correlation ^2
data2.rsquared <- data2.correlation ^2
data3.rsquared <- data3.correlation ^2
data4.rsquared <- data4.correlation ^2
```

```{r, echo=FALSE, results='asis'}
##### DO NOT MODIFY THIS R BLOCK! ######
# This R block will create a table to display all the calculations above in one table.
library(knitr)
library(kableExtra)
results <- data.frame(
	data1.x = c(data1.x.mean, data1.x.median, data1.x.sd, data1.correlation, data1.intercept, data1.slope, data1.rsquared),
	data1.y = c(data1.y.mean, data1.y.median, data1.y.sd, NA, NA, NA, NA),
	data2.x = c(data2.x.mean, data2.x.median, data2.x.sd, data2.correlation, data2.intercept, data2.slope, data2.rsquared),
	data2.y = c(data2.y.mean, data2.y.median, data2.y.sd, NA, NA, NA, NA),
	data3.x = c(data3.x.mean, data3.x.median, data3.x.sd, data3.correlation, data3.intercept, data3.slope, data3.rsquared),
	data3.y = c(data3.y.mean, data3.y.median, data3.y.sd, NA, NA, NA, NA),
	data4.x = c(data4.x.mean, data4.x.median, data4.x.sd, data4.correlation, data4.intercept, data4.slope, data4.rsquared),
	data4.y = c(data4.y.mean, data4.y.median, data4.y.sd, NA, NA, NA, NA),
	stringsAsFactors = FALSE
)
row.names(results) <- c('Mean', 'Median', 'SD', 'r', 'Intercept', 'Slope', 'R-Squared')
names(results) <- c('x','y','x','y','x','y','x','y')
options(knitr.kable.NA = '')
kable(results, digits = 2, 
	  align = 'r',
	  row.names = TRUE, 
	  format.args=list(nsmall = 2)) %>%
	column_spec(2:9, width = ".35in") %>%
	add_header_above(c(" " = 1, "Data 1" = 2, "Data 2" = 2, "Data 3" = 2, "Data 4" = 2))
```

#### g. For each pair, is it appropriate to estimate a linear regression model? Why or why not? Be specific as to why for each pair and include appropriate plots! (4 pts)

Data 1

No, there seems to be positive linear association between the data points. Even though the relationship appears linear in the scatterplot, the residual plot actually shows a nonlinear relationship. This is not a contradiction: residual plots can show divergences from linearity that can be difficult to see in a scatterplot. A simple linear model is inadequate for modeling these data. 

```{r}
par(mfrow=c(2,4))
plot(data1)
plot(data1_lm$residuals)
hist(data1_lm$residuals)
qqnorm(data1_lm$residuals)
qqline(data1_lm$residuals)
```

Data 2

No, the data does not show a linear trend on the scatterplot. The residuals histogram does not follow a normal distribution as well. Therefore, a simple linear model is inadequate for modeling these data. 

```{r}
par(mfrow=c(2,4))
plot(data2)
plot(data2_lm$residuals)
hist(data2_lm$residuals)
qqnorm(data2_lm$residuals)
qqline(data2_lm$residuals)
```

Data 3

No, one of the point is relatively distant from the rest of the data points. This is reflected in the residuals as well. A simple linear model is inadequate for modeling these types data. 

```{r}
par(mfrow=c(2,4))
plot(data3)
plot(data3_lm$residuals)
hist(data3_lm$residuals)
qqnorm(data3_lm$residuals)
qqline(data3_lm$residuals)
```

Data 4

No, the plot does not indicate any linearity at all. The residuals do not follow a normal distribution as well. A simple linear model is inadequate for modeling these data. 

```{r}
par(mfrow=c(2,4))
plot(data4)
plot(data4_lm$residuals)
hist(data4_lm$residuals)
qqnorm(data4_lm$residuals)
qqline(data4_lm$residuals)
```


#### h. Explain why it is important to include appropriate visualizations when analyzing data. Include any visualization(s) you create. (2 pts)

Illustrating the fitted model visually and diagnosing violations of model assumptions can quickly prove/disprove model assumptions. Visualizations make these tasks easier and more conclusive. The evaluations from the above datasets prove this. Dataset 3 seems to fit a linear model until residual analysis prove otherwise.